{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7Y2DPf4YSfpk"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","# -*- author : Vincent Roduit -*-\n","# -*- date : 2023-11-25 -*-\n","# -*- Last revision: 2023-11-25 -*-\n","# -*- python version : 3.11.6 -*-\n","# -*- Description: Notebook that summarize results-*-"]},{"cell_type":"markdown","metadata":{"id":"jz4Q212_00nV"},"source":["# <center> CS -433 Machine Learning </center>\n","## <center> Ecole Polytechnique Fédérale de Lausanne </center>\n","### <center>Road Segmentation </center>\n","---"]},{"cell_type":"markdown","metadata":{"id":"MdLMI0F800nX"},"source":["### Preparing environment for Google Colaboratory"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50392,"status":"ok","timestamp":1701786609498,"user":{"displayName":"Fabio Palmisano","userId":"06730492804394156003"},"user_tz":-60},"id":"T_aLiNuUSj2c","outputId":"8d629b43-8dd6-413a-b6d2-b593786bf19a"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1701786609994,"user":{"displayName":"Fabio Palmisano","userId":"06730492804394156003"},"user_tz":-60},"id":"XP-aN9mZSrUT","outputId":"a233df41-e8bb-4397-b3cf-a8c71577d00e"},"outputs":[],"source":["%cd /content/drive/MyDrive/ml-project-2-team-slo/source"]},{"cell_type":"markdown","metadata":{"id":"RdUktIiT00nZ"},"source":["### Imports"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":6804,"status":"ok","timestamp":1701786634161,"user":{"displayName":"Fabio Palmisano","userId":"06730492804394156003"},"user_tz":-60},"id":"zzABR_GxSfpp"},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["#import libraries\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","from constants import *\n","\n","#import model parameters\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import Adam\n","from torchvision import models\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":11985,"status":"ok","timestamp":1701786647719,"user":{"displayName":"Fabio Palmisano","userId":"06730492804394156003"},"user_tz":-60},"id":"T1rcJzrbSfpq"},"outputs":[],"source":["#import files\n","from data_processing import*\n","from visualization import visualize, visualize_patch\n","import constants\n","from test_data import TestData\n","\n","#import models\n","from cnn import Basic_CNN, Advanced_CNN\n","from logistic_regression import LogisticRegression"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":573,"status":"ok","timestamp":1701786653767,"user":{"displayName":"Fabio Palmisano","userId":"06730492804394156003"},"user_tz":-60},"id":"umywfEaa00na"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x18e6a0193b0>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Set random seed for reproducibility\n","torch.manual_seed(0)"]},{"cell_type":"markdown","metadata":{"id":"FhBB__x900nb"},"source":["## 1. Data wrangling and visualization"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41582,"status":"ok","timestamp":1701786697710,"user":{"displayName":"Fabio Palmisano","userId":"06730492804394156003"},"user_tz":-60},"id":"Z6msnG-K00nb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data...\n","Done!\n","Splitting data...\n","Done!\n","Creating patches...\n","Creating patches for training set...\n","Rotation for 45 degrees\n","Rotation for 135 degrees\n","Rotation for 225 degrees\n","Rotation for 315 degrees\n","Batch 1/15\n","Batch 4/15\n","Batch 7/15\n","Batch 10/15\n","Batch 13/15\n","end process...\n","Creating patches for validation set...\n","Done!\n","Creating dataloader...\n","Done!\n"]}],"source":["myDatas = AdvancedProcessing(standardize=False, num_samples=500)\n","myDatas.proceed()"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data...\n","Done!\n","Standardizing...\n","Done!\n","Formatting data...\n","Done!\n","Creating dataloader...\n","Done!\n"]}],"source":["myTestDatas = TestData(standardize=True)\n","myTestDatas.proceed()"]},{"cell_type":"markdown","metadata":{"id":"SCnJVghs00nc"},"source":["## 2. Define and train models"]},{"cell_type":"markdown","metadata":{"id":"J5woiOoi00nd"},"source":["### 2.1 Logistic regression"]},{"cell_type":"markdown","metadata":{"id":"QaSBjxQu00nd"},"source":["A first attempt could be to try with some linear model. The first approach here is to use a simple logistic regression. In order to use a logistic regression, one need to extract feature from the image. A choice could be to use the mean and the standard deviation as features. The following section will present these approach."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"eF9jpDbw00ne"},"outputs":[{"name":"stderr","output_type":"stream","text":["[autoreload of cnn failed: Traceback (most recent call last):\n","  File \"C:\\Users\\ylaar\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n","    superreload(m, reload, self.old_objects)\n","  File \"C:\\Users\\ylaar\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n","    module = reload(module)\n","             ^^^^^^^^^^^^^^\n","  File \"c:\\Users\\ylaar\\anaconda3\\Lib\\importlib\\__init__.py\", line 169, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"<frozen importlib._bootstrap>\", line 621, in _exec\n","  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n","  File \"c:\\Users\\ylaar\\switchdrive\\epfl\\ml\\ml-project-2-team-slo\\source\\cnn.py\", line 107, in <module>\n","    class Basic_CNN(CNN):\n","  File \"c:\\Users\\ylaar\\switchdrive\\epfl\\ml\\ml-project-2-team-slo\\source\\cnn.py\", line 110, in Basic_CNN\n","    patch_size=constants.WINDOW_SIZE):\n","               ^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: module 'constants' has no attribute 'WINDOW_SIZE'\n","]\n"]},{"name":"stdout","output_type":"stream","text":["Loading data...\n","Done!\n","Creating patches...\n","Done!\n"]}],"source":["LogisticData = BasicProcessing()\n","LogisticData.load_data()\n","LogisticData.create_patches()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PAZYAixI00ne"},"outputs":[],"source":["LogReg = LogisticRegression(LogisticData.imgs_patches, LogisticData.gt_imgs_patches)\n","LogReg.compute_vectors()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1FljFqD00ne"},"outputs":[],"source":["plt.scatter(LogReg.X[:, 0], LogReg.X[:, 1], c=LogReg.Y, edgecolors=\"k\", cmap=plt.cm.Paired)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"CgNnB7J800nf"},"source":["A problem already arises. The datas are not linearly separable. Let's still try to train the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMUqpT5m00nf"},"outputs":[],"source":["LogReg.train()\n","LogReg.predict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vr1328D00nf"},"outputs":[],"source":["print(f'From this model, the accuracy is {LogReg.accuracy*100:.2f}% and the F1 score is {LogReg.f1*100:.2f}%')"]},{"cell_type":"markdown","metadata":{"id":"3WMHBcJ300ng"},"source":["The unsatisfactory results tend us to move to Convolutional Networks, which are more suitable for image datas."]},{"cell_type":"markdown","metadata":{"id":"rOpNxWz900ng"},"source":["### 2.2 Basic Convolutional Neural Network"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Define the loss function\n","criterion = nn.BCEWithLogitsLoss()"]},{"cell_type":"markdown","metadata":{},"source":["#### Training on different patch size"]},{"cell_type":"markdown","metadata":{},"source":["Basic model"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20, Loss: 0.668280284690857, Validation Accuracy: 0.7355, F1 score: nan\n","Epoch 2/20, Loss: 0.6818244309425354, Validation Accuracy: 0.7069, F1 score: nan\n","Epoch 3/20, Loss: 0.6145328700637818, Validation Accuracy: 0.7355, F1 score: nan\n","Epoch 4/20, Loss: 0.702554199180603, Validation Accuracy: 0.6058, F1 score: 0.4335\n","Epoch 5/20, Loss: 0.6768155156326294, Validation Accuracy: 0.6287, F1 score: 0.4181\n","Epoch 6/20, Loss: 0.6486707897186279, Validation Accuracy: 0.6654, F1 score: 0.4607\n","Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 7/20, Loss: 0.6627873078536988, Validation Accuracy: 0.6530, F1 score: 0.4768\n","Epoch 8/20, Loss: 0.6494840443038941, Validation Accuracy: 0.6754, F1 score: 0.4821\n","Epoch 9/20, Loss: 0.6358836765480042, Validation Accuracy: 0.6989, F1 score: 0.4778\n","Epoch 00009: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 10/20, Loss: 0.6388512574768066, Validation Accuracy: 0.6943, F1 score: 0.4833\n","Epoch 11/20, Loss: 0.6409749675178528, Validation Accuracy: 0.6910, F1 score: 0.4862\n","Epoch 12/20, Loss: 0.6410787533950806, Validation Accuracy: 0.6898, F1 score: 0.4855\n","Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 13/20, Loss: 0.6412243668746949, Validation Accuracy: 0.6896, F1 score: 0.4860\n","Epoch 14/20, Loss: 0.6411408095932006, Validation Accuracy: 0.6898, F1 score: 0.4859\n","Epoch 15/20, Loss: 0.6412545517921447, Validation Accuracy: 0.6897, F1 score: 0.4862\n","Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 16/20, Loss: 0.6412759267616271, Validation Accuracy: 0.6896, F1 score: 0.4861\n","Epoch 17/20, Loss: 0.6412802189445496, Validation Accuracy: 0.6896, F1 score: 0.4861\n","Epoch 18/20, Loss: 0.6412908637046814, Validation Accuracy: 0.6896, F1 score: 0.4861\n","Epoch 00018: reducing learning rate of group 0 to 1.0000e-08.\n","Epoch 19/20, Loss: 0.6412913018417359, Validation Accuracy: 0.6896, F1 score: 0.4861\n","Epoch 20/20, Loss: 0.6412915071678161, Validation Accuracy: 0.6896, F1 score: 0.4861\n"]}],"source":["# Define the patch size\n","patch_size = 16\n","\n","# Load data\n","myDatas = AdvancedProcessing(standardize=False, aug_patch_size=patch_size)\n","myDatas.proceed()\n","\n","# Define the model\n","cnn = Basic_CNN(patch_size)\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n","# Define the scheduler\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","# Train the model\n","cnn.train_model(\n","    optimizer,\n","    scheduler,\n","    criterion,\n","    myDatas.train_dataloader,\n","    myDatas.validate_dataloader,\n","    num_epochs=20)\n","\n","torch_file_name = f'basic_cnn_{patch_size}.pth'\n","# Save the model\n","torch.save(cnn, os.path.join(MODELS_DIR, torch_file_name))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["patch_size = 32\n","\n","# Load data\n","myDatas = AdvancedProcessing(standardize=False, aug_patch_size=patch_size)\n","myDatas.proceed()\n","\n","# Define the model\n","cnn = Basic_CNN(patch_size)\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n","# Define the scheduler\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","# Train the model\n","cnn.train_model(\n","    optimizer,\n","    scheduler,\n","    criterion,\n","    myDatas.train_dataloader,\n","    myDatas.validate_dataloader,\n","    num_epochs=20)\n","\n","torch_file_name = f'basic_cnn_{patch_size}.pth'\n","# Save the model\n","torch.save(cnn, os.path.join(MODELS_DIR, torch_file_name))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["patch_size = 64\n","\n","# Load data\n","myDatas = AdvancedProcessing(standardize=False, aug_patch_size=patch_size)\n","myDatas.proceed()\n","\n","# Define the model\n","cnn = Basic_CNN(patch_size)\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n","# Define the scheduler\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","# Train the model\n","cnn.train_model(\n","    optimizer,\n","    scheduler,\n","    criterion,\n","    myDatas.train_dataloader,\n","    myDatas.validate_dataloader,\n","    num_epochs=20)\n","\n","torch_file_name = f'basic_cnn_{patch_size}.pth'\n","# Save the model\n","torch.save(cnn, os.path.join(MODELS_DIR, torch_file_name))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["patch_size = 128\n","\n","# Load data\n","myDatas = AdvancedProcessing(standardize=False, aug_patch_size=patch_size)\n","myDatas.proceed()\n","\n","# Define the model\n","cnn = Basic_CNN(patch_size)\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n","# Define the scheduler\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","# Train the model\n","cnn.train_model(\n","    optimizer,\n","    scheduler,\n","    criterion,\n","    myDatas.train_dataloader,\n","    myDatas.validate_dataloader,\n","    num_epochs=20)\n","\n","torch_file_name = f'basic_cnn_{patch_size}.pth'\n","# Save the model\n","torch.save(cnn, os.path.join(MODELS_DIR, torch_file_name))"]},{"cell_type":"markdown","metadata":{},"source":["Advanced CNN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["patch_size = 64\n","\n","# Load data\n","myDatas = AdvancedProcessing(standardize=False, aug_patch_size=patch_size)\n","myDatas.proceed()\n","\n","# Define the model\n","cnn = Basic_CNN(patch_size)\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n","# Define the scheduler\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","# Train the model\n","cnn.train_model(\n","    optimizer,\n","    scheduler,\n","    criterion,\n","    myDatas.train_dataloader,\n","    myDatas.validate_dataloader,\n","    num_epochs=20)\n","\n","torch_file_name = f'advanced_cnn_{patch_size}.pth'\n","# Save the model\n","torch.save(cnn, os.path.join(MODELS_DIR, torch_file_name))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["patch_size = 128\n","\n","# Load data\n","myDatas = AdvancedProcessing(standardize=False, aug_patch_size=patch_size)\n","myDatas.proceed()\n","\n","# Define the model\n","cnn = Advanced_CNN(patch_size)\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n","# Define the scheduler\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","# Train the model\n","cnn.train_model(\n","    optimizer,\n","    scheduler,\n","    criterion,\n","    myDatas.train_dataloader,\n","    myDatas.validate_dataloader,\n","    num_epochs=20)\n","\n","torch_file_name = f'advanced_cnn_{patch_size}.pth'\n","# Save the model\n","torch.save(cnn, os.path.join(MODELS_DIR, torch_file_name))"]},{"cell_type":"markdown","metadata":{},"source":["#### Color standarization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["patch_size = 128\n","\n","# Load data\n","myDatas = AdvancedProcessing(standardize=True, aug_patch_size=patch_size)\n","myDatas.proceed()\n","\n","# Define the model\n","cnn = Advanced_CNN(patch_size)\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n","# Define the scheduler\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","# Train the model\n","cnn.train_model(\n","    optimizer,\n","    scheduler,\n","    criterion,\n","    myDatas.train_dataloader,\n","    myDatas.validate_dataloader,\n","    num_epochs=20)\n","\n","torch_file_name = f'advanced_cnn_color_{patch_size}.pth'\n","# Save the model\n","torch.save(cnn, os.path.join(MODELS_DIR, torch_file_name))"]},{"cell_type":"markdown","metadata":{},"source":["#### Different optimizer"]},{"cell_type":"markdown","metadata":{},"source":["AdamW"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["patch_size = 128\n","\n","# Load data\n","myDatas = AdvancedProcessing(standardize=False, aug_patch_size=patch_size)\n","myDatas.proceed()\n","\n","# Define the model\n","cnn = Advanced_CNN(patch_size)\n","\n","# Define the optimizer\n","optimizer = torch.optim.AdamW(cnn.parameters(), lr=0.001)\n","# Define the scheduler\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","# Train the model\n","cnn.train_model(\n","    optimizer,\n","    scheduler,\n","    criterion,\n","    myDatas.train_dataloader,\n","    myDatas.validate_dataloader,\n","    num_epochs=20)\n","\n","torch_file_name = f'advanced_cnn_{patch_size}_adamw.pth'\n","# Save the model\n","torch.save(cnn, os.path.join(MODELS_DIR, torch_file_name))"]},{"cell_type":"markdown","metadata":{},"source":["SGD with Nesterov Momentum"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["patch_size = 128\n","\n","# Load data\n","myDatas = AdvancedProcessing(standardize=False, aug_patch_size=patch_size)\n","myDatas.proceed()\n","\n","# Define the model\n","cnn = Advanced_CNN(patch_size)\n","\n","# Define the optimizer\n","optimizer = torch.optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n","# Define the scheduler\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","# Train the model\n","cnn.train_model(\n","    optimizer,\n","    scheduler,\n","    criterion,\n","    myDatas.train_dataloader,\n","    myDatas.validate_dataloader,\n","    num_epochs=20)\n","\n","torch_file_name = f'advanced_cnn_{patch_size}_nesterov.pth'\n","# Save the model\n","torch.save(cnn, os.path.join(MODELS_DIR, torch_file_name))"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
